
# This Week's Top AI/ML Research Papers

#### Vinod

---

#### Transformers without normalization

Transformers can achieve or surpass normalized performance using a simple technique called Dynamic Tanh (DyT), replacing normalization layers with an element-wise operation inspired by tanh-like mappings observed in layer norm, validated across various tasks in computer vision and LLMs.