
# This Week's Top AI/ML Research Papers

#### Vinod

---
<section>
    <h4>Transformers without Normalization</h4>
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: center;">
        <div>
            <p>Transformers can achieve or surpass normalized performance using a simple technique called Dynamic Tanh (DyT), replacing normalization layers with an element-wise operation inspired by tanh-like mappings observed in layer norm, validated across various tasks in computer vision and LLMs.</p>
            <p>
                ðŸ”— <a href="https://arxiv.org/abs/2503.10622">Transformers without Normalization</a>
            </p>
        </div>
        <div>
            <img src="Pasted image 20250325101836.png" alt="Transformers without Normalization" style="width: 100%; max-height: 400px;">
        </div>
    </div>
</section>
---
<section>
    <h4>Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</h4>
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: center;">
        <div>
            <p>Block diffusion language models, merge discrete denoising diffusion with autoregressive models, addressing fixed-length generation limitations and enhancing inference efficiency via KV caching and parallel token sampling. This model introduces a training algorithm, gradient variance estimators, and data-driven noise schedules for minimized variance, achieving state-of-the-art results among diffusion models on benchmarks and enabling flexible-length sequence generation.</p>
            <p>
                ðŸ”— <a href="https://arxiv.org/abs/2503.09573">Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</a>
            </p>
        </div>
        <div>
            <img src="Pasted image 20250325104949.png" alt="Block Diffusion" style="width: 100%; max-height: 400px;">
        </div>
    </div>
</section>
---





