{
	"nodes":[
		{"type":"text","text":"## Illustration of unsupervised, supervised and self-supervised pre-training\n\n![[Pasted image 20250214144312.png]]","id":"e117cbdd6cf9b78c","x":-1160,"y":-1360,"width":927,"height":640},
		{"id":"3f124a613bbf9f7b","x":-1173,"y":-1880,"width":940,"height":280,"type":"text","text":"## Pre-Training of NLP Models\n\n![[Pasted image 20250304072356.png]]\n\nwhere $\\{x_0, x_1, ..., x_m\\}$ denotes a sequence of input tokens, $x_0$ denotes a special symbol ( [s] or  [CLS]) attached to the beginning of a sequence, g(. ; θ) (also written as gθ(·)) denotes a neural network with parameters θ, and o denotes the output of the neural network.\n**Sequence language modeling**: $o$ is a distribution over a vocabulary\n**Sequence encoding problem**:  $o$ is a representation of the input sequence, often expressed as a real-valued vector sequence\n"},
		{"id":"bb08bb50f5012354","x":20,"y":-1600,"width":668,"height":540,"type":"text","text":"### Fine-tuning of Pre-Trained models\n\n![[Pasted image 20250304073520.png]]\n\nText classification system can be expressed as \n![[Pasted image 20250304073557.png]]\n\n![[Pasted image 20250304073723.png]]\n\nExample:\n\n```\nI love the food here. It’s amazing!\n```\n\n![[Pasted image 20250304073903.png]]\n\n"},
		{"id":"fb82c9e50e9c4bc9","x":20,"y":-1020,"width":668,"height":660,"type":"text","text":"### Prompting of Pre-trained Models\n\npre-trained large language models exhibit remarkably good abilities in token prediction, making it possible to transform numerous NLP problems into simple text generation problems through prompting the large language models.\n\n#### zero-shot learning\n![[Pasted image 20250304074307.png]]\n#### in-context learning\n![[Pasted image 20250304074328.png]]\n"},
		{"type":"text","text":"## Common architecture for pre-training Transformer encoders\n\n![[Pasted image 20250214145031.png]]","id":"23b091b49114ebf8","x":-140,"y":-320,"width":1105,"height":776},
		{"id":"2f7fa50d197602da","x":-233,"y":-1180,"width":233,"height":120,"type":"text","text":"techniques for applying these models to downstream tasks after pre-training"},
		{"type":"text","text":"![[Pasted image 20250302211146.png]]","id":"a7a30d2a077619ed","x":-880,"y":1800,"width":600,"height":771},
		{"type":"text","text":"## Permuted Language Modelling\n\n- One  drawback is the use of a special token, [MASK], which is employed only during training but not at test time. \n- These issues can be addressed using the permuted language modeling approach to pretraining.\n- Unlike causal modeling where predictions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language modeling allows for predictions in any order.\n- The probability of the sequence can be modeled via a generation process\n![[Pasted image 20250302195129.png]]\n\nlet us consider a different order for token prediction: x0 → x4 → x2 → x1 → x3.\n![[Pasted image 20250302195058.png]]\n\n![[Pasted image 20250302195217.png]]\n\n- In Transformers, the permutation can be done by setting appropriate masks for self-attention\n![[Pasted image 20250302195545.png]]","id":"55c1018d356ca3ca","x":-425,"y":727,"width":837,"height":940},
		{"type":"text","text":"## Pre-Training Encoders as Classifiers\n\n### 2 popular tasks\n1. Next Sentence Prediction (NSP)\n\n![[Pasted image 20250302210427.png]]\n\n2. ELECTRA: Apply classification based supervision signals to each output of an encoder\n\n![[Pasted image 20250302210910.png]]\n\n\nELECTRA, the maximum likelihood-based loss and the classification-based loss are combined for  jointly training both the generator and discriminator.","id":"d97831a60e0202a8","x":520,"y":727,"width":1117,"height":1266},
		{"type":"text","text":"### Masked Language Modelling\n\n| Masked Language Modeling                                                                                                                             | Causal Language Modeling                                                                                             |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n| all the unmasked tokens are used for word prediction, leading to a bidirectional model that makes predictions based on both left and right-contexts. | only make use of the left-context in word prediction, while the prediction may depend on tokens in the right-context |\nExample:\n$X$ : The early bird catches the worm\n\n$\\bar{X}$: The [MASK] bird catches the [MASK]\n\nthe objective is to maximize the sum of log-scale probabilities\n![[Pasted image 20250302183143.png]]\n\nexample\n![[Pasted image 20250214150606.png]]\n\n\n","id":"468d56c4b4b641cf","x":-1400,"y":727,"width":785,"height":707},
		{"type":"text","text":"## Decoder-only Pre-Training\n\n- Predicts the distribution of tokens at a position given its preceding tokens, and the output is the token with the maximum probability\n- Transformer decoder as a language model by simply removing cross-attention sub-layers from it\n- Given a sequence of $m$ tokens $\\{x_{0}, ..., x_{m}\\}$, the loss on this sequence is the sum of the loss over the positions $\\{0, ..., m - 1\\}$, given by\n![[Pasted image 20250302212236.png]]\n\n\n![[Pasted image 20250302212327.png]]","id":"538dd025789e11d3","x":-1280,"y":-320,"width":941,"height":840},
		{"type":"text","text":"## Encoder-Decoder Pretraining\n\n- encoder-decoder architectures are often used to model sequence-to-sequence problems,  such as machine translation and question answering.\n- Framework of universal language understanding and generation. Both the task instructions and the problem inputs are provided to the system in text form. The system then follows the instructions to complete the task.\n\n```\nSource Text → Target Text\n```\n![[Pasted image 20250302225616.png]]\n\n## Methods to pre-train encoder-decoder models\n\n1. **Prefix language modeling problem**: a language model predicts the subsequent sequence given a prefix, which serves as the context for prediction.\n2. Tokens in a sequence are randomly replaced with a mask symbol, and the model is then trained to predict these masked tokens based on the entire masked sequence\n\n![[Pasted image 20250302224116.png]]\n\n## Denoising Training\n\n![[Pasted image 20250302231355.png]]\n\n![[Pasted image 20250302231420.png]]\n\nAs the model architecture and the training approach have been developed, the remaining issue is the corruption of the input.\n- Token Masking\n- Token Deletion\n  ![[Pasted image 20250302231829.png]]\n- Span Masking\n ![[Pasted image 20250302231904.png]]\n- Sentence Reordering\n ![[Pasted image 20250302231957.png]]\n- Document Rotation\n![[Pasted image 20250302232039.png]]","id":"19db0e32d759c7ed","x":1720,"y":-320,"width":1175,"height":1693},
		{"id":"a342722f823cf959","x":-580,"y":2640,"width":1400,"height":980,"type":"text","text":"## Comparison of Pre-Training Tasks\n\n- **Language Modeling**:  An auto-regressive generation procedure of sequences. At one time, it predicts the next token based on its previous context.\n- **Masked Language Modeling**: It randomly masks tokens in a sequence and predicts these tokens using the entire masked sequence.\n- **Permuted Language Modeling**: It reorders the input sequence and predicts the tokens sequentially. Each prediction is based on some context tokens that are randomly selected.\n- **Discriminative Training**: Models for pre-training are integrated into classifiers and trained together with the remaining parts of the classifiers to enhance their classification performance.\n- **Denoising Autoencoding**: The input is a corrupted sequence and the encoder-decoder models are trained to reconstruct the original sequence.\n\n![[Pasted image 20250304070643.png]]"},
		{"id":"ed10cdd6f071769b","x":-580,"y":3720,"width":1400,"height":720,"type":"text","text":"# BERT\n\n- A Transformer encoder trained using both masked language modeling and next sentence prediction tasks\n![[Pasted image 20250304075510.png]]\n\nThe input representation is a sequence containing two sentences SentA and SentB, expressed as  \n```\n[CLS] SentA [SEP] SentB [SEP]\n```\n![[Pasted image 20250304080441.png]]\n\n"}
	],
	"edges":[
		{"id":"ff35a852fde6427f","fromNode":"468d56c4b4b641cf","fromSide":"bottom","toNode":"a7a30d2a077619ed","toSide":"top"},
		{"id":"9f16c677aae1a103","fromNode":"55c1018d356ca3ca","fromSide":"bottom","toNode":"a7a30d2a077619ed","toSide":"top"},
		{"id":"444348b53e54c05f","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"468d56c4b4b641cf","toSide":"top"},
		{"id":"444e8671e5cc8f3a","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"55c1018d356ca3ca","toSide":"top"},
		{"id":"054982a565d84787","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"d97831a60e0202a8","toSide":"top"},
		{"id":"9f2feee5ddcae01d","fromNode":"3f124a613bbf9f7b","fromSide":"bottom","toNode":"e117cbdd6cf9b78c","toSide":"top","label":"\n1. Optimizing θ on a pre-training task 2. Applying the pre-trained model gθ(·) to downstream tasks \n\n\n"},
		{"id":"1274d5673cd7cd04","fromNode":"e117cbdd6cf9b78c","fromSide":"right","toNode":"bb08bb50f5012354","toSide":"left"},
		{"id":"d877fb08be4eeae3","fromNode":"e117cbdd6cf9b78c","fromSide":"right","toNode":"fb82c9e50e9c4bc9","toSide":"left"}
	]
}