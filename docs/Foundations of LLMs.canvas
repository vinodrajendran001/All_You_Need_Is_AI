{
	"nodes":[
		{"type":"text","text":"## Common architecture for pre-training Transformer encoders\n\n![[Pasted image 20250214145031.png]]","id":"23b091b49114ebf8","x":-140,"y":-680,"width":1105,"height":776},
		{"type":"text","text":"## Permuted Language Modelling\n\n- One  drawback is the use of a special token, [MASK], which is employed only during training but not at test time. \n- These issues can be addressed using the permuted language modeling approach to pretraining.\n- Unlike causal modeling where predictions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language modeling allows for predictions in any order.\n- The probability of the sequence can be modeled via a generation process\n![[Pasted image 20250302195129.png]]\n\nlet us consider a different order for token prediction: x0 → x4 → x2 → x1 → x3.\n![[Pasted image 20250302195058.png]]\n\n![[Pasted image 20250302195217.png]]\n\n- In Transformers, the permutation can be done by setting appropriate masks for self-attention\n![[Pasted image 20250302195545.png]]","id":"55c1018d356ca3ca","x":-380,"y":249,"width":837,"height":940},
		{"type":"text","text":"### Masked Language Modelling\n\n| Masked Language Modeling                                                                                                                             | Causal Language Modeling                                                                                             |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n| all the unmasked tokens are used for word prediction, leading to a bidirectional model that makes predictions based on both left and right-contexts. | only make use of the left-context in word prediction, while the prediction may depend on tokens in the right-context |\nExample:\n$X$ : The early bird catches the worm\n\n$\\bar{X}$: The [MASK] bird catches the [MASK]\n\nthe objective is to maximize the sum of log-scale probabilities\n![[Pasted image 20250302183143.png]]\n\nexample\n![[Pasted image 20250214150606.png]]\n\n\n","id":"468d56c4b4b641cf","x":-1320,"y":249,"width":785,"height":707},
		{"type":"text","text":"## Pre-Training Encoders as Classifiers\n\n### 2 popular tasks\n1. Next Sentence Prediction (NSP)\n\n![[Pasted image 20250302210427.png]]\n\n2. ELECTRA: Apply classification based supervision signals to each output of an encoder\n\n![[Pasted image 20250302210910.png]]\n\n\nELECTRA, the maximum likelihood-based loss and the classification-based loss are combined for  jointly training both the generator and discriminator.","id":"d97831a60e0202a8","x":563,"y":254,"width":1117,"height":1266},
		{"id":"a7a30d2a077619ed","x":-835,"y":1320,"width":600,"height":771,"type":"text","text":"![[Pasted image 20250302211146.png]]"},
		{"id":"538dd025789e11d3","x":-1160,"y":-680,"width":941,"height":840,"type":"text","text":"## Decoder-only Pre-Training\n\n- Predicts the distribution of tokens at a position given its preceding tokens, and the output is the token with the maximum probability\n- Transformer decoder as a language model by simply removing cross-attention sub-layers from it\n- Given a sequence of $m$ tokens $\\{x_{0}, ..., x_{m}\\}$, the loss on this sequence is the sum of the loss over the positions $\\{0, ..., m - 1\\}$, given by\n![[Pasted image 20250302212236.png]]\n\n\n![[Pasted image 20250302212327.png]]"},
		{"type":"text","text":"## Illustration of unsupervised, supervised and self-supervised pre-training\n\n![[Pasted image 20250214144312.png]]","id":"e117cbdd6cf9b78c","x":-1160,"y":-1360,"width":927,"height":640},
		{"id":"19db0e32d759c7ed","x":-155,"y":2007,"width":1035,"height":913,"type":"text","text":"## Encoder-Decoder Pretraining\n\n- encoder-decoder architectures are often used to model sequence-to-sequence problems,  such as machine translation and question answering.\n- Framework of universal language understanding and generation. Both the task instructions and the problem inputs are provided to the system in text form. The system then follows the instructions to complete the task.\n\n```\nSource Text → Target Text\n```\n![[Pasted image 20250302225616.png]]\n\n## Methods to pre-train encoder-decoder models\n\n1. **Prefix language modeling problem**: a language model predicts the subsequent sequence given a prefix, which serves as the context for prediction.\n2. Tokens in a sequence are randomly replaced with a mask symbol, and the model is then trained to predict these masked tokens based on the entire masked sequence\n\n![[Pasted image 20250302224116.png]]\n\n## Denoising Training\n\n![[Pasted image 20250302231355.png]]\n\n![[Pasted image 20250302231420.png]]\n\nAs the model architecture and the training approach have been developed, the remaining issue is the corruption of the input.\n- Token Masking\n- Token Deletion\n  ![[Pasted image 20250302231829.png]]\n  \n- Span Masking\n- ![[Pasted image 20250302231904.png]]\n- \n- Sentence Reordering\n- Document Rotation\n"}
	],
	"edges":[
		{"id":"ff35a852fde6427f","fromNode":"468d56c4b4b641cf","fromSide":"bottom","toNode":"a7a30d2a077619ed","toSide":"top"},
		{"id":"9f16c677aae1a103","fromNode":"55c1018d356ca3ca","fromSide":"bottom","toNode":"a7a30d2a077619ed","toSide":"top"},
		{"id":"444348b53e54c05f","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"468d56c4b4b641cf","toSide":"top"},
		{"id":"444e8671e5cc8f3a","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"55c1018d356ca3ca","toSide":"top"},
		{"id":"054982a565d84787","fromNode":"23b091b49114ebf8","fromSide":"bottom","toNode":"d97831a60e0202a8","toSide":"top"}
	]
}