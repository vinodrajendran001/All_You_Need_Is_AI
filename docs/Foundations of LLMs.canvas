{
	"nodes":[
		{"id":"5cdf6368689a54a7","x":-1960,"y":-657,"width":840,"height":1197,"type":"group","label":"Types of Pre-Training"},
		{"type":"text","text":"Pre-training NLP Models","id":"a45d205b0af755b8","x":-740,"y":-300,"width":250,"height":60,"color":"1"},
		{"type":"text","text":"## Simplified Representation\n\n$o = g(x_0,x_1,...,x_m; \\theta)$\n$= g_{ \\theta} (x_0, x_1,...,x_m)$\n\nwhere\n${x_0,x_1,...,x_m}$ $->$ sequence of input tokens\n$x_0$ $->$ special symbol $<s>$ or $[CLS]$\n$g(. ;\\theta)$ (or $g_{\\theta}(.)$) $->$ neural network parameters $\\theta$\n$o$ $->$ output of the neural network (vary based on the problem)\n\n***Example:*** \n\n$o$ $->$ distribution of vocabulary for language modelling; \n$o$ $->$ representation of input sequence (real valued vector space)\n","id":"0c432c434edb1b1a","x":-360,"y":-520,"width":545,"height":370},
		{"type":"text","text":"## Fundamental Issues\n\n- **Optimizing $\\theta$ on pre-training task** - as focus is on training a model that can generalize across various tasks instead of specific downstream task.\n\n- **Applying the pre-trained $g_\\theta(.)$ to downstream tasks** - To adapt the model to the specifc tasks we need to adjust the $\\hat\\theta$ ","id":"f66d28dc60a71590","x":240,"y":-463,"width":485,"height":257},
		{"type":"text","text":"## Self-supervised Pre-Training\n\nThe neural network is trained using the supervision signals generated by itself, rather than provided by humans.","id":"6bd71f48a9dcae82","x":-1940,"y":330,"width":426,"height":160},
		{"type":"text","text":"## Supervised Pre-Training\n\n**Pre-training**: A sequence model designed to encode input sequences into some representations. The model is then combined with a layer to form a supervised-learning system $->$ **new model**\n\n**Fine-tuning**: the parameters $\\theta$ of new neural network model using task-specific labeled data.","id":"dd633393caa4ab2a","x":-1940,"y":2,"width":426,"height":277},
		{"type":"text","text":"## Unsupervised Pre-Training\n\nThe parameters $\\theta$ of neural network are optimized using a criterion that is not directly related to specific tasks.","id":"564d6a3d1d3890f1","x":-1940,"y":-270,"width":366,"height":155},
		{"type":"text","text":"![[Pasted image 20250208202252.png]]","id":"0fd45d69be180c41","x":-1940,"y":-637,"width":458,"height":337},
		{"type":"text","text":" ## Sequence Modelling/ Encoding\n- Given a sequence of words or tokens, a sequence encoding model represents this sequence as either a real-valued vector or a sequence of vectors, and obtains a representation of the sequence.","id":"14558561f7d4c3f8","x":-740,"y":2,"width":380,"height":258},
		{"type":"text","text":"## Sequence Generation\n\n- Refers to the problem of generating a sequence of tokens based on a given context. ","id":"05377bcccc39747f","x":-740,"y":320,"width":380,"height":180},
		{"id":"f0761203d1b72ff1","x":-1974,"y":580,"width":920,"height":900,"type":"text","text":" # Fine-tuning of Pre-trained Models\n\nLet $Encoder_\\theta(.)$ denote an encoder with parameters $\\theta$, \nwhere $Encoder_\\theta(.)$ can be a standard **Transformer** encoder.\n\nAssume, we have pre-trained this model and obtained the optimatal paramers $\\hat\\theta$. \nNow we can employ it to model any sequence and generate corresponding representation.\n\n$H = Encode_\\hat\\theta(.)$\n\nwhere,\n$x$ is the input sequence $\\{x_0,x_1,..x_m\\}$,\n$H$ is the output representation of real-valued vectors $\\{h_1, h_2,...,h_m\\}$\n\nWe can build a classification system by stacking a **classifier** on top of the **encoder**\n![[Pasted image 20250208204300.png]]\n\n$F_{ω,\\hat\\theta}(·)$  to denote $Classify_{ω}(Encode_{\\hat\\theta}(·))$\n\nthe model parameters $ω$ and $\\hat\\theta$ are not optimized for the classification task, we cannot  \ndirectly use this model.\n\nA typical way is to fine-tune the model by giving explicit labeling in downstream tasks.  \nWe can train $F_{ω,\\hat\\theta}(·)$ on a labeled dataset, treating it as a common supervised learning task. The outcome of the fine-tuning is the parameters $\\tilde ω,\\tilde\\theta$  that are further optimized.\n\n\n### Example:\n\n**Input text** : I love the food here. It's amazing!\n**Tokenization**: {I, love, the, food, here, ., It, ’s, amazing, !}\n**Model**:\n![[Pasted image 20250208205207.png]]\n**Output**: Positive\n\n\n\n\n\n\n"},
		{"id":"d38e821c8df559a2","x":-1020,"y":580,"width":1000,"height":900,"type":"text","text":"## Prompting of Pre-trained Models\n\n- sequence generation models are often employed independently to address language generation problems, such as question answering and machine translation, without the need for additional modules.\n\n- pre-trained large language models exhibit remarkably good abilities in token prediction, making it possible to transform numerous NLP problems into simple text generation problems through prompting the large language models.\n\n```\nI love the food here. It’s amazing! I’m _______\n```\n\nHere `_______` indicates the word or phrase we want to predict (call it the **completion**). If the predicted\nword is happy, or glad, or satisfied or a related positive word, we can classify the text as *positive*.\n\nHere is a prompt where  we prompt the LLM to perform polarity classification with an instruction.\n\n```\nAssume that the polarity of a text is a label chosen from {positive, negative,  \nneutral}. Identify the polarity of the input.  \nInput: I love the food here. It’s amazing!  \nPolarity: ______\n```\n\nBy using instruction-based prompts, we can adapt large language models to  solve NLP problems without the need for additional training. $->$ **Zero-shot prompting**\n\nwe can add some samples that demonstrate how  an input corresponds to an output $->$ Made possible by **Incontext Learning**\nThese samples, known as demonstrations, are used to teach  large language models how to perform the task. $->$ **Few shot learning**\n\n```\nAssume that the polarity of a text is a label chosen from {positive, negative,  \nneutral}. Identify the polarity of the input.  \nInput: The traffic is terrible during rush hours, making it difficult to reach the  \nairport on time.  \nPolarity: Negative  \nInput: The weather here is wonderful.  \nPolarity: Positive  \nInput: I love the food here. It’s amazing!  \nPolarity:________\n```\n"}
	],
	"edges":[
		{"id":"b4b169c5dbd5ded7","fromNode":"0c432c434edb1b1a","fromSide":"right","toNode":"f66d28dc60a71590","toSide":"left"},
		{"id":"a495fa5947b3ae9b","fromNode":"a45d205b0af755b8","fromSide":"right","toNode":"0c432c434edb1b1a","toSide":"left"},
		{"id":"ad730866bf767871","fromNode":"a45d205b0af755b8","fromSide":"left","toNode":"14558561f7d4c3f8","toSide":"left"},
		{"id":"af7a59120b21c86b","fromNode":"a45d205b0af755b8","fromSide":"left","toNode":"05377bcccc39747f","toSide":"left"},
		{"id":"3bd741ce65fbdcbf","fromNode":"564d6a3d1d3890f1","fromSide":"right","toNode":"dd633393caa4ab2a","toSide":"right","label":"Makes the supervised label easier and more stable"},
		{"id":"ea7ed01f31e2ca8f","fromNode":"a45d205b0af755b8","fromSide":"top","toNode":"5cdf6368689a54a7","toSide":"top"}
	]
}