{
	"nodes":[
		{"type":"text","text":"## Illustration of unsupervised, supervised and self-supervised pre-training\n\n![[Pasted image 20250214144312.png]]","id":"e117cbdd6cf9b78c","x":-740,"y":-600,"width":720,"height":560},
		{"type":"text","text":"## Common architecture for pre-training Transformer encoders\n\n![[Pasted image 20250214145031.png]]","id":"23b091b49114ebf8","x":-740,"y":-20,"width":805,"height":576},
		{"type":"text","text":"### Masked Language Modelling\n\n| Masked Language Modeling                                                                                                                             | Causal Language Modeling                                                                                             |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n| all the unmasked tokens are used for word prediction, leading to a bidirectional model that makes predictions based on both left and right-contexts. | only make use of the left-context in word prediction, while the prediction may depend on tokens in the right-context |\nExample:\n$X$ : The early bird catches the worm\n\n$\\bar{X}$: The [MASK] bird catches the [MASK]\n\nthe objective is to maximize the sum of log-scale probabilities\n![[Pasted image 20250302183143.png]]\n\nexample\n![[Pasted image 20250214150606.png]]\n\n\n","id":"468d56c4b4b641cf","x":180,"y":-20,"width":785,"height":707},
		{"id":"55c1018d356ca3ca","x":-737,"y":600,"width":802,"height":469,"type":"text","text":""}
	],
	"edges":[]
}