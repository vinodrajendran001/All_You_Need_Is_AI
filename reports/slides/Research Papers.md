
# This Week's Top AI/ML Research Papers

#### Vinod

---
<section>
    <h4>Transformers without Normalization</h4>
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: center;">
        <div>
            <p>Transformers can achieve or surpass normalized performance using a simple technique called Dynamic Tanh (DyT), replacing normalization layers with an element-wise operation inspired by tanh-like mappings observed in layer norm, validated across various tasks in computer vision and LLMs.</p>
            <p>
                ðŸ”— <a href="https://arxiv.org/abs/2503.10622">Transformers without Normalization</a>
            </p>
        </div>
        <div>
            <img src="Pasted image 20250325101836.png" alt="Transformers without Normalization" style="width: 100%; max-height: 400px;">
        </div>
    </div>
</section>
---





