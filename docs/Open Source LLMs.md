
Architecture Commonalities

1. Pre-training objective: Next-token prediction
2. Transformers architecture: decoder-only transformer as their foundation
3. Tokenization: BPE, sentence-piece
4. Self-attention mechanism
5. Layer Normalization
6. Fully-Feed forward network
7. Residual connections
8. Training pipeline