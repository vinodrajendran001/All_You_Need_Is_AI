{
	"nodes":[
		{"id":"8c66c015bc8f1712","type":"text","text":"## LLMs scaling to new heights through reasoning frameworks.\n\nDeepSeek-R1\nOpenAI o3-mini\nGoogle gemini 2.0 flash Thinking","x":-700,"y":-440,"width":400,"height":200,"color":"1"},
		{"id":"210081a468d05850","type":"text","text":"## Paradigm shift from scaling **train-time compute** to scaling **test-time compute**\n\n![[Pasted image 20250205112357.png]]","x":-280,"y":-440,"width":640,"height":540},
		{"id":"8739dbba5037caa7","type":"text","text":"## What are reasoning LLMs?\n\nBreakdown a problem into smaller steps before answering the question\n\t- Reasoning steps/ thought process / Chain-of-thought","x":-700,"y":180,"width":400,"height":220,"color":"1"},
		{"id":"20a180d423f3e0c7","type":"text","text":"![[Pasted image 20250205113223.png]]","x":420,"y":180,"width":560,"height":400},
		{"id":"f222d1efe8856c68","type":"text","text":"![[Pasted image 20250205112923.png]]","x":-280,"y":180,"width":640,"height":400},
		{"id":"05832a77d05f812e","type":"text","text":"## What is **train-time** compute?\n\nUntil half of 2024, to increase the performance of LLMs during **pre-training**, developers often increase the **size** of the:\n\n- Model (# of **parameters**)\n- Dataset (# of **tokens**)\n- Compute (# of **FLOPs**)","x":-700,"y":760,"width":400,"height":320,"color":"1"},
		{"id":"2ae4561d75b841aa","type":"text","text":"![[Pasted image 20250205114759.png]]","x":-280,"y":760,"width":605,"height":260},
		{"id":"1605f728607ca743","type":"text","text":"Are LLMs actually able to think like humans?\n- Psychologist feels it focus too much on following human bevaviour. ","x":100,"y":610,"width":405,"height":120,"color":"3"},
		{"id":"dc388a24e543ec32","type":"text","text":"Instead of having LLMs learn **what** to answer they learn **how** to answer!","x":760,"y":610,"width":405,"height":120,"color":"3"},
		{"id":"218d556b5fab7e45","type":"text","text":"## Paradigm shift from **train-time compute** to **test-time compute**","x":-760,"y":640,"width":520,"height":90,"color":"1"},
		{"id":"c1b345d6a2c186e9","type":"text","text":"The larger your pretraining budget, the better the resulting model will be","x":100,"y":1080,"width":405,"height":80,"color":"3"},
		{"id":"a9df61bc1394392b","type":"text","text":"![[Pasted image 20250205131233.png]]","x":420,"y":760,"width":685,"height":215},
		{"id":"82bf02f6f907dee6","type":"text","text":"Train-time compute include both the compute needed during training as well as during fine-tuning","x":838,"y":1020,"width":327,"height":100,"color":"3"},
		{"id":"3bf86bcdca01c72e","type":"text","text":"## Scaling Laws\n\nHow a model’s scale (through compute, dataset size, and model size) correlates with a model’s performance","x":-700,"y":1260,"width":400,"height":180,"color":"1"},
		{"id":"7195e5e54e6a1423","type":"text","text":"![[Pasted image 20250205132135.png]]","x":-280,"y":1260,"width":724,"height":333},
		{"id":"d2bbb7599e4107cd","type":"text","text":"Typically shown in a log--log scale (which results in a straight line) to showcase the large increase in compute.","x":100,"y":1640,"width":405,"height":80,"color":"3"},
		{"id":"cf7de2c326ee1598","type":"text","text":"## Kaplan and Chinchilla scaling laws\n![[Pasted image 20250205132354.png]]","x":480,"y":1260,"width":864,"height":360},
		{"id":"fef30629a476c6ae","type":"text","text":"These laws more and less say that the performance of a model will increase with more compute, tokens, and parameters.","x":1002,"y":1680,"width":405,"height":80,"color":"3"},
		{"id":"060714cde9a8c45d","type":"text","text":"Kaplan's Scaling Law states that scaling the model's size is typically more effective than scaling data (given fixed compute).","x":1002,"y":1800,"width":405,"height":80,"color":"3"},
		{"id":"9a074fe37bb054d8","type":"text","text":"Chinchilla Scaling Law suggests that the model size and data are equally important.","x":1002,"y":1920,"width":405,"height":80,"color":"3"},
		{"id":"bb3dd97477a0400e","type":"text","text":"## Have we reached a wall?\n\nthroughout 2024, compute, dataset size and model parameters have steadily grown, yet the gains have shown diminishing returns.\n\n![[Pasted image 20250205133021.png]]","x":1460,"y":1260,"width":786,"height":519},
		{"id":"971a148c9dc724df","type":"text","text":"## What is **test-time** compute?\n\nInstead of continuously increasing pre-training budgets, test-time compute allows models to **think longer** during i**inference**","x":-700,"y":2040,"width":400,"height":180,"color":"1"},
		{"id":"95283e4b37068df3","type":"text","text":"![[Pasted image 20250205133319.png]]","x":-280,"y":2040,"width":724,"height":440},
		{"id":"6583f7ba2c5e8c62","type":"text","text":"## Non-Reasoning models would normally only output the answer and skip any **reasoning** step.\n\n![[Pasted image 20250205133659.png]]","x":480,"y":2040,"width":864,"height":574},
		{"id":"6874fef4f82d5333","type":"text","text":"## Reasoning models would instead use more tokens to derive their answer through a systematic **thinking** process:\n\n![[Pasted image 20250205134042.png]]","x":1460,"y":2040,"width":1088,"height":854},
		{"id":"6428f49c8620060a","type":"text","text":"## Idea: using more effort to think deeply and give a better answer like brainstorming before writing a final response.\n\n![[Pasted image 20250205134624.png]]","x":2600,"y":2040,"width":1120,"height":854},
		{"id":"46edbd8cfa4848fc","type":"text","text":"## Scaling Laws\n\nCompared to train-time compute, scaling laws for test-time compute are relatively new.","x":-700,"y":3000,"width":400,"height":180,"color":"1"},
		{"id":"bb3f697d65e74393","type":"text","text":"## OpenAI showcasing that test-time compute might actually follow the same trend as scaling train-time compute.\n\n![[Pasted image 20250205135406.png]]","x":-265,"y":3000,"width":709,"height":560},
		{"id":"277aa8101cb803a0","type":"text","text":"## \"Scaling Scaling Laws with Board Games\" explores AlphaZero and trains it to various degrees of compute to play Hex.\n\n\n![[Pasted image 20250205135900.png]]\n\n\n","x":480,"y":3000,"width":1485,"height":905},
		{"id":"d6068f5d9c5ee7f2","type":"text","text":"## Their results show that **train-time** and **test-time** compute are **tightly related**.\n\n![[Pasted image 20250205140035.png]]","x":480,"y":4040,"width":1485,"height":1120},
		{"id":"cb5b2982634ced4e","type":"text","text":"# As the test-time compute scaling is like train-time compute, a paradigm shift is happening toward \"reasoning\" models using more test-time compute","x":2004,"y":3000,"width":760,"height":140,"color":"2"},
		{"id":"4a81832cb8f48049","type":"text","text":"# Instead of focusing purely on train-time compute (pre-training and fine-tuning), these “reasoning” models instead balance training with inference.\n\n![[Pasted image 20250205140805.png]]","x":2004,"y":3313,"width":760,"height":487,"color":"2"},
		{"id":"5655ad84f059bd2c","type":"text","text":"## Test-time compute could even scale in length:\n\n![[Pasted image 20250205140956.png]]","x":2800,"y":3000,"width":1485,"height":762},
		{"id":"930b73bedddaf0c1","type":"text","text":"## Categories of Test-Time Compute\n\nThe incredible success of reasoning models like DeepSeek R-1 and OpenAI o1 showcase that there are more techniques than simply thinking **longer**.\n\nTest-time compute can be many different things, including **Chain-of-Thought, revising answers, backtracking, sampling, and more**.\n\nGrouped into 2 categories\n- Search against Verifiers (sampling generations and selecting the best answer)\n- Modifying Proposal Distribution (trained “thinking” process)","x":-700,"y":5280,"width":480,"height":420,"color":"1"},
		{"id":"458636d68e32a5c0","type":"text","text":"![[Pasted image 20250205144335.png]]","x":-160,"y":5280,"width":1200,"height":880},
		{"id":"fc4ab7eec4973668","type":"text","text":"# Search against verifiers is **output-focused** whereas modifying the proposal distribution is **input-focused**.\n\n![[Pasted image 20250205145524.png]]","x":1120,"y":5280,"width":1322,"height":606},
		{"id":"6985a10b5b63540e","type":"text","text":"## Two types of verifiers\n\n- Outcome Reward Models (ORM)\n- Process Reward Models (PRM)","x":2520,"y":5503,"width":337,"height":160,"color":"1"},
		{"id":"b4e0ba216b9063fc","type":"text","text":"# ORM\n\n## The ORM only judges the outcome and doesn’t care about the underlying process:\n\n![[Pasted image 20250205145918.png]]","x":3240,"y":4600,"width":1160,"height":940},
		{"id":"99f0802e831cf53b","type":"text","text":"# PRM\n\n## PRM judges the process that leads to the outcome (the \"reasoning\")\n\n\n![[Pasted image 20250205151402.png]]\n\nTo make these reasoning steps a bit more explicit:\n\n![[Pasted image 20250205151447.png]]\n\nNote how step 2 is a poor reasoning step and is scored low by the PRM!","x":3240,"y":5620,"width":1160,"height":1140},
		{"id":"d6fce9f9f949cc4e","type":"text","text":"## Search against Verifiers \n\n1. First, multiple samples of reasoning processes and answers are created.\n2. Second, a verifier (Reward Model) scores the generated output","x":-700,"y":6960,"width":480,"height":240,"color":"1"}
	],
	"edges":[
		{"id":"cbe8bfe14866cd5b","fromNode":"f222d1efe8856c68","fromSide":"bottom","toNode":"1605f728607ca743","toSide":"left"},
		{"id":"84a5985eb90ac497","fromNode":"20a180d423f3e0c7","fromSide":"bottom","toNode":"dc388a24e543ec32","toSide":"left"},
		{"id":"3fe72d59755158ff","fromNode":"218d556b5fab7e45","fromSide":"left","toNode":"05832a77d05f812e","toSide":"left"},
		{"id":"194662b6d946f95e","fromNode":"2ae4561d75b841aa","fromSide":"bottom","toNode":"c1b345d6a2c186e9","toSide":"left"},
		{"id":"8f46715af078f8c1","fromNode":"a9df61bc1394392b","fromSide":"bottom","toNode":"82bf02f6f907dee6","toSide":"left"},
		{"id":"8544dc4581ed9198","fromNode":"7195e5e54e6a1423","fromSide":"bottom","toNode":"d2bbb7599e4107cd","toSide":"left"},
		{"id":"c75cc7c45f8c0465","fromNode":"cf7de2c326ee1598","fromSide":"bottom","toNode":"fef30629a476c6ae","toSide":"left"},
		{"id":"d550443d47dbb597","fromNode":"cf7de2c326ee1598","fromSide":"bottom","toNode":"060714cde9a8c45d","toSide":"left"},
		{"id":"9a4b3b40f78ef528","fromNode":"cf7de2c326ee1598","fromSide":"bottom","toNode":"9a074fe37bb054d8","toSide":"left"},
		{"id":"43a284788506db8a","fromNode":"218d556b5fab7e45","fromSide":"left","toNode":"971a148c9dc724df","toSide":"left"},
		{"id":"cea1fda2fd64c478","fromNode":"05832a77d05f812e","fromSide":"bottom","toNode":"3bf86bcdca01c72e","toSide":"top"},
		{"id":"4c7dbca9378f13a1","fromNode":"971a148c9dc724df","fromSide":"bottom","toNode":"46edbd8cfa4848fc","toSide":"top"},
		{"id":"19b86c855eeb2b60","fromNode":"277aa8101cb803a0","fromSide":"bottom","toNode":"d6068f5d9c5ee7f2","toSide":"top"},
		{"id":"123259bbc3dac30b","fromNode":"cb5b2982634ced4e","fromSide":"bottom","toNode":"4a81832cb8f48049","toSide":"top"},
		{"id":"210d8e4b577c096c","fromNode":"971a148c9dc724df","fromSide":"left","toNode":"930b73bedddaf0c1","toSide":"left"},
		{"id":"c76dc7e94db40773","fromNode":"6985a10b5b63540e","fromSide":"right","toNode":"b4e0ba216b9063fc","toSide":"left"},
		{"id":"3606806e59b0d82f","fromNode":"6985a10b5b63540e","fromSide":"right","toNode":"99f0802e831cf53b","toSide":"left"},
		{"id":"192aaa7d7854323e","fromNode":"930b73bedddaf0c1","fromSide":"bottom","toNode":"d6fce9f9f949cc4e","toSide":"top"}
	]
}